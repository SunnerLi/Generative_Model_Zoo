# --- Setting for BigGAN optimizer ---
#
# Differ to paper, we alter to use AdamW because of attention block in UNet2DModel
# Adam with GAN + UNet2DModel would fail to train
optimizer_cls: torch.optim.AdamW
betas: [0.0, 0.999]
lr_g: 0.0001
lr_d: 0.0001
weight_decay: 0.01