# --- Setting for BigGAN optimizer ---
#
# Differ to paper, we alter to use AdamW because of attention block in UNet2DModel
# Adam with GAN + UNet2DModel would fail to train
optimizer_cls: torch.optim.AdamW
betas: [0.0, 0.999]
lr: 0.0002
weight_decay: 0.01